import torch
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import trackio

# ========== 1. åˆå§‹åŒ– TrackIO ==========
trackio.init(project="Ai-nlp")

# æ¨¡æ‹Ÿå¾ªç¯ 3 æ¬¡è¯„ä¼°ï¼ˆæ¯”å¦‚ä½ å¯ä»¥æ”¾åœ¨ epoch åé¢ï¼‰
for eval_round in range(1, 4):
    print(f"ğŸ” Evaluation round {eval_round}")

    # ========== 2. æ¨¡å‹é¢„æµ‹ ==========
    preds, labels = [], []
    for row in test_df.to_dict(orient="records"):
        inputs = tokenizer(row["prompt"], return_tensors="pt").to(model.device)
        outputs = model.generate(**inputs, max_new_tokens=5)
        pred = tokenizer.decode(outputs[0], skip_special_tokens=True).split("Category:")[-1].strip()
        preds.append(pred if pred in ["AI", "NLP"] else "AI")  # fallback é¿å…ç©ºå€¼
        labels.append(row["completion"])

    # ========== 3. è®¡ç®—ä¸»è¦æŒ‡æ ‡ ==========
    acc = accuracy_score(labels, preds)
    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average="weighted")
    metrics = {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1}

    # åŠ ä¸Š eval_round ä½œä¸º step
    trackio.log({"step": eval_round, **metrics})

    # ========== 4. æ¯ç±»æŒ‡æ ‡ï¼ˆAI / NLPï¼‰ ==========
    prec_c, rec_c, f1_c, _ = precision_recall_fscore_support(labels, preds, labels=["AI", "NLP"])
    trackio.log({
        "step": eval_round,
        "AI_precision": prec_c[0],
        "AI_recall": rec_c[0],
        "AI_f1": f1_c[0],
        "NLP_precision": prec_c[1],
        "NLP_recall": rec_c[1],
        "NLP_f1": f1_c[1],
    })

    # ========== 5. æ··æ·†çŸ©é˜µä½œä¸º artifact ==========
    cm = confusion_matrix(labels, preds, labels=["AI", "NLP"])
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["AI", "NLP"], yticklabels=["AI", "NLP"])
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"Confusion Matrix - Eval round {eval_round}")
    fname = f"confusion_matrix_{eval_round}.png"
    plt.savefig(fname)
    plt.close()
    trackio.log_artifact(fname)

    # ========== 6. åˆ†ç±»æŠ¥å‘Šä½œä¸º artifact ==========
    report_txt = classification_report(labels, preds, target_names=["AI", "NLP"])
    fname = f"classification_report_{eval_round}.txt"
    with open(fname, "w", encoding="utf-8") as f:
        f.write(report_txt)
    trackio.log_artifact(fname)

# ========== 7. å®Œæˆ ==========
trackio.finish()
print("âœ… è¯„ä¼°å®Œæˆï¼Œå·²æ‰§è¡Œ 3 æ¬¡ï¼Œæ‰€æœ‰æ•°æ®å†™å…¥ TrackIO (Ai-nlp é¡¹ç›®)ã€‚")
