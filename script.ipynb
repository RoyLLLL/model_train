{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69828b-bb1c-48a5-a74a-8d913437caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre deal dataset\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# 1. 加载 BANKING77 数据集（训练集和测试集）\n",
    "dataset = load_dataset(\"banking77\")\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]\n",
    "\n",
    "# 2. 定义映射表：原始意图名称 -> Agent 类型\n",
    "intent_to_agent = {\n",
    "    \"activate_my_card\": \"common\",       \"age_limit\": \"common\",\n",
    "    \"apple_pay_or_google_pay\": \"option\", \"atm_support\": \"common\",\n",
    "    \"automatic_top_up\": \"common\",\n",
    "    \"balance_not_updated_after_bank_transfer\": \"common\",\n",
    "    \"balance_not_updated_after_cheque_or_cash_deposit\": \"cash\",\n",
    "    \"beneficiary_not_allowed\": \"common\", \"cancel_transfer\": \"common\",\n",
    "    \"card_about_to_expire\": \"common\",   \"card_acceptance\": \"common\",\n",
    "    \"card_arrival\": \"common\",          \"card_delivery_estimate\": \"common\",\n",
    "    \"card_linking\": \"common\",          \"card_not_working\": \"common\",\n",
    "    \"card_payment_fee_charged\": \"common\",\n",
    "    \"card_payment_not_recognised\": \"common\",\n",
    "    \"card_payment_wrong_exchange_rate\": \"cash\",\n",
    "    \"card_swallowed\": \"common\",        \"cash_withdrawal_charge\": \"cash\",\n",
    "    \"cash_withdrawal_not_recognised\": \"cash\",\n",
    "    \"change_pin\": \"common\",            \"compromised_card\": \"common\",\n",
    "    \"contactless_not_working\": \"option\",\"country_support\": \"common\",\n",
    "    \"declined_card_payment\": \"common\",\"declined_cash_withdrawal\": \"cash\",\n",
    "    \"declined_transfer\": \"common\",     \"direct_debit_payment_not_recognised\": \"common\",\n",
    "    \"disposable_card_limits\": \"common\",\"edit_personal_details\": \"common\",\n",
    "    \"exchange_charge\": \"cash\",         \"exchange_rate\": \"equity\",\n",
    "    \"exchange_via_app\": \"em\",          \"extra_charge_on_statement\": \"common\",\n",
    "    \"failed_transfer\": \"common\",       \"fiat_currency_support\": \"cash\",\n",
    "    \"get_disposable_virtual_card\": \"em\",\"get_physical_card\": \"common\",\n",
    "    \"getting_spare_card\": \"common\",    \"getting_virtual_card\": \"em\",\n",
    "    \"lost_or_stolen_card\": \"common\",   \"lost_or_stolen_phone\": \"common\",\n",
    "    \"order_physical_card\": \"common\",   \"passcode_forgotten\": \"common\",\n",
    "    \"pending_card_payment\": \"common\",  \"pending_cash_withdrawal\": \"cash\",\n",
    "    \"pending_top_up\": \"common\",        \"pending_transfer\": \"common\",\n",
    "    \"pin_blocked\": \"common\",           \"receiving_money\": \"common\",\n",
    "    \"Refund_not_showing_up\": \"common\", \"request_refund\": \"common\",\n",
    "    \"reverted_card_payment\": \"common\", \"supported_cards_and_currencies\": \"equity\",\"reverted_card_payment?\": \"common\",\n",
    "    \"terminate_account\": \"common\",     \"top_up_by_bank_transfer_charge\": \"common\",\n",
    "    \"top_up_by_card_charge\": \"common\", \"top_up_by_cash_or_cheque\": \"cash\",\n",
    "    \"top_up_failed\": \"common\",         \"top_up_limits\": \"common\",\n",
    "    \"top_up_reverted\": \"common\",       \"topping_up_by_card\": \"common\",\n",
    "    \"transaction_charged_twice\": \"common\",\n",
    "    \"transfer_fee_charged\": \"common\",  \"transfer_into_account\": \"common\",\n",
    "    \"transfer_not_received_by_recipient\": \"common\", \"transfer_timing\": \"common\",\n",
    "    \"unable_to_verify_identity\": \"common\", \"verify_my_identity\": \"common\",\n",
    "    \"verify_source_of_funds\": \"common\",\"verify_top_up\": \"common\",\n",
    "    \"virtual_card_not_working\": \"em\",   \"visa_or_mastercard\": \"option\",\n",
    "    \"why_verify_identity\": \"common\",   \"wrong_amount_of_cash_received\": \"cash\",\n",
    "    \"wrong_exchange_rate_for_cash_withdrawal\": \"cash\"\n",
    "}\n",
    "\n",
    "# 3. 将数据集中每条样本映射为对话格式：用户提问 + 提示 + 机器人类型\n",
    "def convert_to_agent_example(ex):\n",
    "    intent_name = train_ds.features[\"label\"].int2str(ex[\"label\"])  # 将数字标签转为名称\n",
    "    agent = intent_to_agent[intent_name]  # 查表得到 Agent 类型\n",
    "    user_text = ex[\"text\"] + \"\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.\"\n",
    "    # 构建符合 Llama 对话微调格式的 JSON 结构\n",
    "    return {\"conversations\": [\n",
    "        {\"role\": \"user\",    \"content\": user_text},\n",
    "        {\"role\": \"assistant\",\"content\": agent}\n",
    "    ]}\n",
    "\n",
    "# 对训练集和测试集分别处理\n",
    "train_conv = train_ds.map(lambda ex: convert_to_agent_example(ex))\n",
    "test_conv  = test_ds.map(lambda ex: convert_to_agent_example(ex))\n",
    "\n",
    "\n",
    "with open(\"banking77_agent_train.jsonl\", \"w\", encoding=\"utf-8\") as ftr:\n",
    "    for ex in train_conv:\n",
    "        ftr.write(json.dumps({\"conversations\": ex[\"conversations\"]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# 保存测试集\n",
    "with open(\"banking77_agent_test.jsonl\", \"w\", encoding=\"utf-8\") as fte:\n",
    "    for ex in test_conv:\n",
    "        fte.write(json.dumps({\"conversations\": ex[\"conversations\"]}, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7f25c5-d1d9-4ee8-96a3-a8d4753c56fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA 版本: 12.4\n",
      "CUDA 是否可用: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch CUDA 版本:\", torch.version.cuda)  # 应显示 12.4\n",
    "print(\"CUDA 是否可用:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac7e423-4b39-4480-bf8b-841f64e2ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/roy/models/datasets_cache'\n",
    "os.environ['HF_HOME'] = '/home/roy/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d567b6ea-1f6c-46c8-a069-48a539aefa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3019/3945427878.py:3: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n",
    "\n",
    "train_data = load_dataset(\"json\", data_files={\"train\": \"banking77_agent_train.jsonl\"})[\"train\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/roy/models/models--unsloth--Llama-3.2-3B-Instruct\")\n",
    "# 3. 加载处理好的对话训练数据\n",
    "train_data = load_dataset(\"json\", data_files={\"train\": \"banking77_agent_train.jsonl\"})[\"train\"]\n",
    "# 使用 unsloth 的 sharegpt 标准化（生成 'conversations' 字段为统一格式）\n",
    "train_data = standardize_sharegpt(train_data)\n",
    "\n",
    "# 4. 格式化输入文本：将对话转换为纯文本序列\n",
    "def format_for_training(batch):\n",
    "    texts = []\n",
    "    for convo in batch[\"conversations\"]:\n",
    "        # unsloth 的 apply_chat_template 将对话转换为带有分隔标记的输入文本\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "train_data = train_data.map(format_for_training, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1536f9-c563-40da-9a63-3c27ba27888e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'I am still waiting on my card?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'common', 'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 12 Jul 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI am still waiting on my card?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ncommon<|eot_id|>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "321df362-18a6-4a3f-a033-0d96834e389b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'Why has my new card still not come?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'common', 'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 12 Jul 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhy has my new card still not come?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ncommon<|eot_id|>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e50aee-e35f-4ff7-8d24-d17a21996cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=28): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10003/10003 [00:03<00:00, 3075.96 examples/s]\n",
      "Map (num_proc=28): 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10003/10003 [00:00<00:00, 42398.26 examples/s]\n",
      "/home/roy/miniconda3/envs/ai_train/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.3226005935668945, metrics={'train_runtime': 61.1685, 'train_samples_per_second': 13.079, 'train_steps_per_second': 1.635, 'total_flos': 1008007295041536.0, 'train_loss': 0.3226005935668945})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n",
    "import os\n",
    "\n",
    "# 1. 加载预训练的 Llama-3.2-3B-Instruct（8-bit 量化以节省显存）\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/roy/models/models--unsloth--Llama-3.2-3B-Instruct\",\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. 配置 LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)  # 将 LoRA 层注入模型\n",
    "\n",
    "\n",
    "# 5. 配置 SFTTrainer（监督微调）\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_8bit\",\n",
    "        max_steps=100,           # 训练步数根据数据量酌情设置\n",
    "        output_dir=\"llama3_finetune\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 6. 仅在回答部分进行训练\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7930ac5c-1661-475f-b1d0-5c358c02a617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama3-lora/tokenizer_config.json',\n",
       " './llama3-lora/special_tokens_map.json',\n",
       " './llama3-lora/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存整个 PEFT 模型（LoRA 层）\n",
    "trainer.model.save_pretrained(\"./llama3-lora\")\n",
    "\n",
    "# 同时保存 tokenizer（必要）\n",
    "tokenizer.save_pretrained(\"./llama3-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f34c3bc-efd2-488e-8d9f-4c18752bf690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/roy/models/models--unsloth--Llama-3.2-3B-Instruct\",\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "traind_tokenizer = AutoTokenizer.from_pretrained(\"./llama3-lora\")\n",
    "\n",
    "# Step 3: 加载 LoRA adapter 权重\n",
    "traind_model = PeftModel.from_pretrained(base_model, \"./llama3-lora\")\n",
    "# 将模型切换到 评估模式（evaluation mode）\n",
    "traind_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45c4ea06-0444-4230-838e-599f356c8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = load_dataset(\"json\", data_files={\"train\": \"banking77_agent_test.jsonl\"})['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd1f573d-ba49-48b2-b9c6-f1f9bafe9154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'How do I locate my card?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'common', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5224ccff-1aec-41b9-af98-18bc44a6ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Accuracy: 86.95%\n",
      "🔍 前5个预测错误样例：\n",
      "\n",
      "📝 输入: How often do your exchange rates change\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "✅ 真实: equity\n",
      "❌ 预测: common\n",
      "\n",
      "📝 输入: what are exchange rates\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "✅ 真实: equity\n",
      "❌ 预测: cash\n",
      "\n",
      "📝 输入: What are the most current exchange rates?\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "✅ 真实: equity\n",
      "❌ 预测: common\n",
      "\n",
      "📝 输入: Can you explain your exchange rate policy to me?\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "✅ 真实: equity\n",
      "❌ 预测: common\n",
      "\n",
      "📝 输入: Is it a good time to exchange?\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "✅ 真实: equity\n",
      "❌ 预测: common\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def predict_agent_type(user_input):\n",
    "    # 构造消息\n",
    "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    \n",
    "    # 使用 tokenizer 处理输入，返回 PyTorch 张量\n",
    "    encodings = traind_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 直接使用 encodings 作为 input_ids，并移动到模型设备\n",
    "    input_ids = encodings.to(traind_model.device)\n",
    "    \n",
    "    # 生成输出\n",
    "    with torch.no_grad():\n",
    "        outputs = traind_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            temperature=1.0\n",
    "        )\n",
    "    \n",
    "    # 解码输出\n",
    "    reply = traind_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return reply.strip().split('assistant')[1].strip()\n",
    "\n",
    "# 假设 traind_model 和 traind_tokenizer 已正确加载\n",
    "# 示例加载方式（需根据实际情况调整）：\n",
    "# traind_tokenizer = AutoTokenizer.from_pretrained(\"path/to/model\")\n",
    "# traind_model = AutoModelForCausalLM.from_pretrained(\"path/to/model\")\n",
    "# traind_model.eval()\n",
    "\n",
    "# 遍历测试集并评估准确率\n",
    "correct = 0\n",
    "total = 0\n",
    "wrong_samples = []\n",
    "\n",
    "for ex in test_ds:\n",
    "    user_input = ex[\"conversations\"][0][\"content\"]  # 用户输入\n",
    "    true_label = ex[\"conversations\"][1][\"content\"]  # 真实标签\n",
    "\n",
    "    pred_label = predict_agent_type(user_input)\n",
    "    total += 1\n",
    "    if pred_label.lower() == true_label.lower():\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong_samples.append((user_input, true_label, pred_label))\n",
    "\n",
    "# 打印准确率与错误样例\n",
    "accuracy = correct / total\n",
    "print(f\"\\n✅ Accuracy: {accuracy:.2%}\")\n",
    "print(\"🔍 前5个预测错误样例：\")\n",
    "for text, true, pred in wrong_samples[:5]:\n",
    "    print(f\"\\n📝 输入: {text}\\n✅ 真实: {true}\\n❌ 预测: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36103e55-19b9-4dc4-9c22-d05bfb2249f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48c31dc4-33c0-447f-b491-a4e3e70080ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 12 Jul 2025\n",
      "\n",
      "user\n",
      "\n",
      "I am still waiting on my card? Please answer using exactly one of the following types: equity, option, cash, em, common.assistant\n",
      "\n",
      "common\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'common'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_agent_type(user_input):\n",
    "    # 构造消息\n",
    "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    \n",
    "    # 使用 tokenizer 处理输入，返回 PyTorch 张量\n",
    "    encodings = traind_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 直接使用 encodings 作为 input_ids，并移动到模型设备\n",
    "    input_ids = encodings.to(traind_model.device)\n",
    "    \n",
    "    # 生成输出\n",
    "    with torch.no_grad():\n",
    "        outputs = traind_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            temperature=1.0\n",
    "        )\n",
    "    \n",
    "    # 解码输出\n",
    "    reply = traind_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(reply)\n",
    "    # 返回第一个词作为预测结果\n",
    "    return reply.strip().split('assistant')[1].strip()\n",
    "\n",
    "user_input = 'I am still waiting on my card? Please answer using exactly one of the following types: equity, option, cash, em, common.'\n",
    "predict_agent_type(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb92e4-8c0b-493f-8206-15b64688a3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
