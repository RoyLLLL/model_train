{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69828b-bb1c-48a5-a74a-8d913437caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre deal dataset\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# 1. åŠ è½½ BANKING77 æ•°æ®é›†ï¼ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰\n",
    "dataset = load_dataset(\"banking77\")\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds = dataset[\"test\"]\n",
    "\n",
    "# 2. å®šä¹‰æ˜ å°„è¡¨ï¼šåŸå§‹æ„å›¾åç§° -> Agent ç±»å‹\n",
    "intent_to_agent = {\n",
    "    \"activate_my_card\": \"common\",       \"age_limit\": \"common\",\n",
    "    \"apple_pay_or_google_pay\": \"option\", \"atm_support\": \"common\",\n",
    "    \"automatic_top_up\": \"common\",\n",
    "    \"balance_not_updated_after_bank_transfer\": \"common\",\n",
    "    \"balance_not_updated_after_cheque_or_cash_deposit\": \"cash\",\n",
    "    \"beneficiary_not_allowed\": \"common\", \"cancel_transfer\": \"common\",\n",
    "    \"card_about_to_expire\": \"common\",   \"card_acceptance\": \"common\",\n",
    "    \"card_arrival\": \"common\",          \"card_delivery_estimate\": \"common\",\n",
    "    \"card_linking\": \"common\",          \"card_not_working\": \"common\",\n",
    "    \"card_payment_fee_charged\": \"common\",\n",
    "    \"card_payment_not_recognised\": \"common\",\n",
    "    \"card_payment_wrong_exchange_rate\": \"cash\",\n",
    "    \"card_swallowed\": \"common\",        \"cash_withdrawal_charge\": \"cash\",\n",
    "    \"cash_withdrawal_not_recognised\": \"cash\",\n",
    "    \"change_pin\": \"common\",            \"compromised_card\": \"common\",\n",
    "    \"contactless_not_working\": \"option\",\"country_support\": \"common\",\n",
    "    \"declined_card_payment\": \"common\",\"declined_cash_withdrawal\": \"cash\",\n",
    "    \"declined_transfer\": \"common\",     \"direct_debit_payment_not_recognised\": \"common\",\n",
    "    \"disposable_card_limits\": \"common\",\"edit_personal_details\": \"common\",\n",
    "    \"exchange_charge\": \"cash\",         \"exchange_rate\": \"equity\",\n",
    "    \"exchange_via_app\": \"em\",          \"extra_charge_on_statement\": \"common\",\n",
    "    \"failed_transfer\": \"common\",       \"fiat_currency_support\": \"cash\",\n",
    "    \"get_disposable_virtual_card\": \"em\",\"get_physical_card\": \"common\",\n",
    "    \"getting_spare_card\": \"common\",    \"getting_virtual_card\": \"em\",\n",
    "    \"lost_or_stolen_card\": \"common\",   \"lost_or_stolen_phone\": \"common\",\n",
    "    \"order_physical_card\": \"common\",   \"passcode_forgotten\": \"common\",\n",
    "    \"pending_card_payment\": \"common\",  \"pending_cash_withdrawal\": \"cash\",\n",
    "    \"pending_top_up\": \"common\",        \"pending_transfer\": \"common\",\n",
    "    \"pin_blocked\": \"common\",           \"receiving_money\": \"common\",\n",
    "    \"Refund_not_showing_up\": \"common\", \"request_refund\": \"common\",\n",
    "    \"reverted_card_payment\": \"common\", \"supported_cards_and_currencies\": \"equity\",\"reverted_card_payment?\": \"common\",\n",
    "    \"terminate_account\": \"common\",     \"top_up_by_bank_transfer_charge\": \"common\",\n",
    "    \"top_up_by_card_charge\": \"common\", \"top_up_by_cash_or_cheque\": \"cash\",\n",
    "    \"top_up_failed\": \"common\",         \"top_up_limits\": \"common\",\n",
    "    \"top_up_reverted\": \"common\",       \"topping_up_by_card\": \"common\",\n",
    "    \"transaction_charged_twice\": \"common\",\n",
    "    \"transfer_fee_charged\": \"common\",  \"transfer_into_account\": \"common\",\n",
    "    \"transfer_not_received_by_recipient\": \"common\", \"transfer_timing\": \"common\",\n",
    "    \"unable_to_verify_identity\": \"common\", \"verify_my_identity\": \"common\",\n",
    "    \"verify_source_of_funds\": \"common\",\"verify_top_up\": \"common\",\n",
    "    \"virtual_card_not_working\": \"em\",   \"visa_or_mastercard\": \"option\",\n",
    "    \"why_verify_identity\": \"common\",   \"wrong_amount_of_cash_received\": \"cash\",\n",
    "    \"wrong_exchange_rate_for_cash_withdrawal\": \"cash\"\n",
    "}\n",
    "\n",
    "# 3. å°†æ•°æ®é›†ä¸­æ¯æ¡æ ·æœ¬æ˜ å°„ä¸ºå¯¹è¯æ ¼å¼ï¼šç”¨æˆ·æé—® + æç¤º + æœºå™¨äººç±»å‹\n",
    "def convert_to_agent_example(ex):\n",
    "    intent_name = train_ds.features[\"label\"].int2str(ex[\"label\"])  # å°†æ•°å­—æ ‡ç­¾è½¬ä¸ºåç§°\n",
    "    agent = intent_to_agent[intent_name]  # æŸ¥è¡¨å¾—åˆ° Agent ç±»å‹\n",
    "    user_text = ex[\"text\"] + \"\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.\"\n",
    "    # æ„å»ºç¬¦åˆ Llama å¯¹è¯å¾®è°ƒæ ¼å¼çš„ JSON ç»“æ„\n",
    "    return {\"conversations\": [\n",
    "        {\"role\": \"user\",    \"content\": user_text},\n",
    "        {\"role\": \"assistant\",\"content\": agent}\n",
    "    ]}\n",
    "\n",
    "# å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«å¤„ç†\n",
    "train_conv = train_ds.map(lambda ex: convert_to_agent_example(ex))\n",
    "test_conv  = test_ds.map(lambda ex: convert_to_agent_example(ex))\n",
    "\n",
    "\n",
    "with open(\"banking77_agent_train.jsonl\", \"w\", encoding=\"utf-8\") as ftr:\n",
    "    for ex in train_conv:\n",
    "        ftr.write(json.dumps({\"conversations\": ex[\"conversations\"]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ä¿å­˜æµ‹è¯•é›†\n",
    "with open(\"banking77_agent_test.jsonl\", \"w\", encoding=\"utf-8\") as fte:\n",
    "    for ex in test_conv:\n",
    "        fte.write(json.dumps({\"conversations\": ex[\"conversations\"]}, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7f25c5-d1d9-4ee8-96a3-a8d4753c56fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA ç‰ˆæœ¬: 12.4\n",
      "CUDA æ˜¯å¦å¯ç”¨: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch CUDA ç‰ˆæœ¬:\", torch.version.cuda)  # åº”æ˜¾ç¤º 12.4\n",
    "print(\"CUDA æ˜¯å¦å¯ç”¨:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac7e423-4b39-4480-bf8b-841f64e2ec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/roy/models/datasets_cache'\n",
    "os.environ['HF_HOME'] = '/home/roy/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d567b6ea-1f6c-46c8-a069-48a539aefa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3019/3945427878.py:3: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n",
    "\n",
    "train_data = load_dataset(\"json\", data_files={\"train\": \"banking77_agent_train.jsonl\"})[\"train\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/roy/models/models--unsloth--Llama-3.2-3B-Instruct\")\n",
    "# 3. åŠ è½½å¤„ç†å¥½çš„å¯¹è¯è®­ç»ƒæ•°æ®\n",
    "train_data = load_dataset(\"json\", data_files={\"train\": \"banking77_agent_train.jsonl\"})[\"train\"]\n",
    "# ä½¿ç”¨ unsloth çš„ sharegpt æ ‡å‡†åŒ–ï¼ˆç”Ÿæˆ 'conversations' å­—æ®µä¸ºç»Ÿä¸€æ ¼å¼ï¼‰\n",
    "train_data = standardize_sharegpt(train_data)\n",
    "\n",
    "# 4. æ ¼å¼åŒ–è¾“å…¥æ–‡æœ¬ï¼šå°†å¯¹è¯è½¬æ¢ä¸ºçº¯æ–‡æœ¬åºåˆ—\n",
    "def format_for_training(batch):\n",
    "    texts = []\n",
    "    for convo in batch[\"conversations\"]:\n",
    "        # unsloth çš„ apply_chat_template å°†å¯¹è¯è½¬æ¢ä¸ºå¸¦æœ‰åˆ†éš”æ ‡è®°çš„è¾“å…¥æ–‡æœ¬\n",
    "        text = tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "train_data = train_data.map(format_for_training, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1536f9-c563-40da-9a63-3c27ba27888e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'I am still waiting on my card?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'common', 'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 12 Jul 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI am still waiting on my card?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ncommon<|eot_id|>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "321df362-18a6-4a3f-a033-0d96834e389b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'Why has my new card still not come?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'common', 'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 12 Jul 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhy has my new card still not come?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\ncommon<|eot_id|>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e50aee-e35f-4ff7-8d24-d17a21996cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=28): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10003/10003 [00:03<00:00, 3075.96 examples/s]\n",
      "Map (num_proc=28): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10003/10003 [00:00<00:00, 42398.26 examples/s]\n",
      "/home/roy/miniconda3/envs/ai_train/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.3226005935668945, metrics={'train_runtime': 61.1685, 'train_samples_per_second': 13.079, 'train_steps_per_second': 1.635, 'total_flos': 1008007295041536.0, 'train_loss': 0.3226005935668945})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from unsloth.chat_templates import standardize_sharegpt, train_on_responses_only\n",
    "import os\n",
    "\n",
    "# 1. åŠ è½½é¢„è®­ç»ƒçš„ Llama-3.2-3B-Instructï¼ˆ8-bit é‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜ï¼‰\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/roy/models/models--unsloth--Llama-3.2-3B-Instruct\",\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2. é…ç½® LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)  # å°† LoRA å±‚æ³¨å…¥æ¨¡å‹\n",
    "\n",
    "\n",
    "# 5. é…ç½® SFTTrainerï¼ˆç›‘ç£å¾®è°ƒï¼‰\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_8bit\",\n",
    "        max_steps=100,           # è®­ç»ƒæ­¥æ•°æ ¹æ®æ•°æ®é‡é…Œæƒ…è®¾ç½®\n",
    "        output_dir=\"llama3_finetune\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 6. ä»…åœ¨å›ç­”éƒ¨åˆ†è¿›è¡Œè®­ç»ƒ\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7930ac5c-1661-475f-b1d0-5c358c02a617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama3-lora/tokenizer_config.json',\n",
       " './llama3-lora/special_tokens_map.json',\n",
       " './llama3-lora/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¿å­˜æ•´ä¸ª PEFT æ¨¡å‹ï¼ˆLoRA å±‚ï¼‰\n",
    "trainer.model.save_pretrained(\"./llama3-lora\")\n",
    "\n",
    "# åŒæ—¶ä¿å­˜ tokenizerï¼ˆå¿…è¦ï¼‰\n",
    "tokenizer.save_pretrained(\"./llama3-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f34c3bc-efd2-488e-8d9f-4c18752bf690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/roy/models/models--unsloth--Llama-3.2-3B-Instruct\",\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "traind_tokenizer = AutoTokenizer.from_pretrained(\"./llama3-lora\")\n",
    "\n",
    "# Step 3: åŠ è½½ LoRA adapter æƒé‡\n",
    "traind_model = PeftModel.from_pretrained(base_model, \"./llama3-lora\")\n",
    "# å°†æ¨¡å‹åˆ‡æ¢åˆ° è¯„ä¼°æ¨¡å¼ï¼ˆevaluation modeï¼‰\n",
    "traind_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45c4ea06-0444-4230-838e-599f356c8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = load_dataset(\"json\", data_files={\"train\": \"banking77_agent_test.jsonl\"})['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd1f573d-ba49-48b2-b9c6-f1f9bafe9154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'How do I locate my card?\\nPlease answer using exactly one of the following types: equity, option, cash, em, common.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'common', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5224ccff-1aec-41b9-af98-18bc44a6ee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Accuracy: 86.95%\n",
      "ğŸ” å‰5ä¸ªé¢„æµ‹é”™è¯¯æ ·ä¾‹ï¼š\n",
      "\n",
      "ğŸ“ è¾“å…¥: How often do your exchange rates change\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "âœ… çœŸå®: equity\n",
      "âŒ é¢„æµ‹: common\n",
      "\n",
      "ğŸ“ è¾“å…¥: what are exchange rates\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "âœ… çœŸå®: equity\n",
      "âŒ é¢„æµ‹: cash\n",
      "\n",
      "ğŸ“ è¾“å…¥: What are the most current exchange rates?\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "âœ… çœŸå®: equity\n",
      "âŒ é¢„æµ‹: common\n",
      "\n",
      "ğŸ“ è¾“å…¥: Can you explain your exchange rate policy to me?\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "âœ… çœŸå®: equity\n",
      "âŒ é¢„æµ‹: common\n",
      "\n",
      "ğŸ“ è¾“å…¥: Is it a good time to exchange?\n",
      "Please answer using exactly one of the following types: equity, option, cash, em, common.\n",
      "âœ… çœŸå®: equity\n",
      "âŒ é¢„æµ‹: common\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def predict_agent_type(user_input):\n",
    "    # æ„é€ æ¶ˆæ¯\n",
    "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    \n",
    "    # ä½¿ç”¨ tokenizer å¤„ç†è¾“å…¥ï¼Œè¿”å› PyTorch å¼ é‡\n",
    "    encodings = traind_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # ç›´æ¥ä½¿ç”¨ encodings ä½œä¸º input_idsï¼Œå¹¶ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡\n",
    "    input_ids = encodings.to(traind_model.device)\n",
    "    \n",
    "    # ç”Ÿæˆè¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        outputs = traind_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            temperature=1.0\n",
    "        )\n",
    "    \n",
    "    # è§£ç è¾“å‡º\n",
    "    reply = traind_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return reply.strip().split('assistant')[1].strip()\n",
    "\n",
    "# å‡è®¾ traind_model å’Œ traind_tokenizer å·²æ­£ç¡®åŠ è½½\n",
    "# ç¤ºä¾‹åŠ è½½æ–¹å¼ï¼ˆéœ€æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰ï¼š\n",
    "# traind_tokenizer = AutoTokenizer.from_pretrained(\"path/to/model\")\n",
    "# traind_model = AutoModelForCausalLM.from_pretrained(\"path/to/model\")\n",
    "# traind_model.eval()\n",
    "\n",
    "# éå†æµ‹è¯•é›†å¹¶è¯„ä¼°å‡†ç¡®ç‡\n",
    "correct = 0\n",
    "total = 0\n",
    "wrong_samples = []\n",
    "\n",
    "for ex in test_ds:\n",
    "    user_input = ex[\"conversations\"][0][\"content\"]  # ç”¨æˆ·è¾“å…¥\n",
    "    true_label = ex[\"conversations\"][1][\"content\"]  # çœŸå®æ ‡ç­¾\n",
    "\n",
    "    pred_label = predict_agent_type(user_input)\n",
    "    total += 1\n",
    "    if pred_label.lower() == true_label.lower():\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong_samples.append((user_input, true_label, pred_label))\n",
    "\n",
    "# æ‰“å°å‡†ç¡®ç‡ä¸é”™è¯¯æ ·ä¾‹\n",
    "accuracy = correct / total\n",
    "print(f\"\\nâœ… Accuracy: {accuracy:.2%}\")\n",
    "print(\"ğŸ” å‰5ä¸ªé¢„æµ‹é”™è¯¯æ ·ä¾‹ï¼š\")\n",
    "for text, true, pred in wrong_samples[:5]:\n",
    "    print(f\"\\nğŸ“ è¾“å…¥: {text}\\nâœ… çœŸå®: {true}\\nâŒ é¢„æµ‹: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36103e55-19b9-4dc4-9c22-d05bfb2249f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48c31dc4-33c0-447f-b491-a4e3e70080ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 12 Jul 2025\n",
      "\n",
      "user\n",
      "\n",
      "I am still waiting on my card? Please answer using exactly one of the following types: equity, option, cash, em, common.assistant\n",
      "\n",
      "common\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'common'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_agent_type(user_input):\n",
    "    # æ„é€ æ¶ˆæ¯\n",
    "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "    \n",
    "    # ä½¿ç”¨ tokenizer å¤„ç†è¾“å…¥ï¼Œè¿”å› PyTorch å¼ é‡\n",
    "    encodings = traind_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # ç›´æ¥ä½¿ç”¨ encodings ä½œä¸º input_idsï¼Œå¹¶ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡\n",
    "    input_ids = encodings.to(traind_model.device)\n",
    "    \n",
    "    # ç”Ÿæˆè¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        outputs = traind_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False,\n",
    "            temperature=1.0\n",
    "        )\n",
    "    \n",
    "    # è§£ç è¾“å‡º\n",
    "    reply = traind_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(reply)\n",
    "    # è¿”å›ç¬¬ä¸€ä¸ªè¯ä½œä¸ºé¢„æµ‹ç»“æœ\n",
    "    return reply.strip().split('assistant')[1].strip()\n",
    "\n",
    "user_input = 'I am still waiting on my card? Please answer using exactly one of the following types: equity, option, cash, em, common.'\n",
    "predict_agent_type(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb92e4-8c0b-493f-8206-15b64688a3eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
