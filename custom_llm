from typing import List
from langchain_core.language_models.chat import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.outputs import ChatResult, ChatGeneration
import requests


class VLLMChat(BaseChatModel):
    def __init__(self, endpoint: str, model: str):
        super().__init__()
        self._endpoint = endpoint
        self._model = model

    def _generate(
        self,
        messages: List[BaseMessage],
        stop=None,
        run_manager=None,
        **kwargs,
    ) -> ChatResult:
        # 转换 LangChain 的消息到 OpenAI 格式
        payload = {
            "model": self._model,
            "messages": [{"role": m.type, "content": m.content} for m in messages],
            "chat_template_kwargs": {"enable_thinking": False},  # 🔑 关键参数
        }
        resp = requests.post(self._endpoint, json=payload)
        resp.raise_for_status()
        data = resp.json()

        output_message = data["choices"][0]["message"]["content"]

        # 手动构造 ChatResult
        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=output_message))]
        )

    @property
    def _llm_type(self) -> str:
        return "vllm-chat"


# ✅ 用法
if __name__ == "__main__":
    llm = VLLMChat("http://localhost:8000/v1/chat/completions", "AI-NLP")
    response = llm.invoke([HumanMessage(content="介绍一下 LangChain 的用途")])
    print(response.content)
