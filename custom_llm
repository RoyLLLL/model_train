from typing import List
from langchain_core.language_models.chat import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.outputs import ChatResult, ChatGeneration
import requests


class VLLMChat(BaseChatModel):
    def __init__(self, endpoint: str, model: str):
        super().__init__()
        self._endpoint = endpoint
        self._model = model

    def _generate(
        self,
        messages: List[BaseMessage],
        stop=None,
        run_manager=None,
        **kwargs,
    ) -> ChatResult:
        # è½¬æ¢ LangChain çš„æ¶ˆæ¯åˆ° OpenAI æ ¼å¼
        payload = {
            "model": self._model,
            "messages": [{"role": m.type, "content": m.content} for m in messages],
            "chat_template_kwargs": {"enable_thinking": False},  # ğŸ”‘ å…³é”®å‚æ•°
        }
        resp = requests.post(self._endpoint, json=payload)
        resp.raise_for_status()
        data = resp.json()

        output_message = data["choices"][0]["message"]["content"]

        # æ‰‹åŠ¨æ„é€  ChatResult
        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=output_message))]
        )

    @property
    def _llm_type(self) -> str:
        return "vllm-chat"


# âœ… ç”¨æ³•
if __name__ == "__main__":
    llm = VLLMChat("http://localhost:8000/v1/chat/completions", "AI-NLP")
    response = llm.invoke([HumanMessage(content="ä»‹ç»ä¸€ä¸‹ LangChain çš„ç”¨é€”")])
    print(response.content)
