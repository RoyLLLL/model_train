
from transformers import BertTokenizer, BertForSequenceClassification
import os

os.environ['HF_DATASETS_CACHE'] = 'D:/transformers/models/datasets_cache'
os.environ['HF_HOME'] = 'D:/transformers/models'

# 指定保存模型的路径
model_path = "./output/dir/checkpoint-3378"  # 替换为实际保存的checkpoint路径

# 加载模型和分词器
tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = BertForSequenceClassification.from_pretrained(model_path)

text = "track my card while it  the process of delive"  # 替换为你的输入文本

# 使用与训练相同的预处理方式
inputs = tokenizer(
    text,
    padding="max_length",  # 保持与训练一致的参数
    truncation=True,
    max_length=128,
    return_tensors="pt"    # 返回PyTorch张量
)

import torch

# 设置为评估模式
model.eval()

with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class_id = logits.argmax().item()

from datasets import load_dataset
dataset = load_dataset("banking77")

label_names = dataset["train"].features["label"].names

# 打印结果
print(f"输入文本: {text}")
print(f"预测类别ID: {predicted_class_id}")
print(f"对应标签: {label_names[predicted_class_id]}")